agent: TD3
agent_kwargs:
  tau: 0.005
  policy_noise: 0.1
  target_noise: 0.2
  noise_clip: 0.5
  critic_freq: 1
  actor_freq: 2
  target_freq: 2
  init_steps: 25000

  network_class: ActorCriticPolicy
  network_kwargs:
    actor_class: ContinuousMLPActor
    actor_kwargs:
      hidden_layers: [256, 256]
      output_act: ["import", "torch.nn", "Tanh"]
    critic_class: ContinuousMLPCritic
    critic_kwargs:
      hidden_layers: [256, 256]
      num_q_fns: 2

  dataset_class: ReplayBuffer
  dataset_kwargs:
    capacity: 1000000
    batch_size: null
    sample_strategy: uniform
    save_frequency: 1000

  optimizer_class: Adam
  optimizer_kwargs:
    lr: 0.00005

train_kwargs: # Arguments given to Algorithm.train
  total_steps: 250000 # The total number of steps to train
  log_freq: 250 # How often to log values
  eval_freq: 10000 # How often to run evals
  eval_ep: 10 # Number of enviornment episodes to run for evaluation, or -1 if none should be run.
  loss_metric: reward # The validation metric that determines when to save the "best_checkpoint"
  workers: 4 # Number of dataloader workers.
  profile_freq: 50
